<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<!-- saved from url=(0040)http://people.eecs.berkeley.edu/~barron/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    a {
    color: #1772d0;
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }
    body,td,th,tr,p,a {
    font-family: Verdana, Helvetica, sans-serif;
    font-size: 18px
    }
    strong {
    font-family: Verdana, Helvetica, sans-serif;
    font-size: 18px;
    }
    heading {
    font-family: Verdana, Helvetica, sans-serif;
    font-size: 22px;
    }
    papertitle {
    font-family: Verdana, Helvetica, sans-serif;
    font-size: 18px;
    font-weight: 700
    }
    name {
    font-family: Verdana, Helvetica, sans-serif;
    font-size: 32px;
    }
    .one
    {
    width: 160px;
    height: 160px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }
  </style>
  
  <title>Aseem</title>
  
  <link href="./website_files/css" rel="stylesheet" type="text/css">
  </head>
  <body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tbody><tr>
    <td>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tbody><tr>
        <td width="67%" valign="middle">
        <p align="center">
          <name>Aseem Saxena</name>
        </p>
		<p> I am currently pursuing my Masters in Robotics at Oregon State University. I am fortunate to be advised by <a href="https://web.engr.oregonstate.edu/~afern/">Prof Alan Fern</a> at the <a href="https://mime.oregonstate.edu/research/drl/">Dynamic Robotics Lab</a>.</p> 
	<p>Until very recently, I worked as a Machine Learning Engineer at Panasonic, Singapore.</p>
	<p> Before that, I was working as a Researcher at <a href="http://motion.comp.nus.edu.sg/">M2AP Lab, School of Computing, NUS</a>, Singapore under the guidance of <a href="http://www.comp.nus.edu.sg/~dyhsu/">Prof. David Hsu</a>.</p>
			
        <p>
          I did a Dual Major in Biology and Electrical And Electronics Engineering at <a href="http://www.bits-pilani.ac.in/">BITS Pilani</a>, my education was funded by the <a href="http://kvpy.iisc.ernet.in/main/index.htm">KVPY Fellowship</a>. I spent time on my thesis at <a href="http://robotics.iiit.ac.in/">Robotics Research Lab, IIIT Hyderabad</a>, where I was advised by <a href="https://faculty.iiit.ac.in/~mkrishna/">Prof Madhava Krishna</a>. 
        </p>
        <p align="center">
<a href="https://robotvisionxyz.github.io">Blog</a> &nbsp;/&nbsp;		
<a href="mailto:aseem.bits@gmail.com">Email</a> &nbsp;/&nbsp;
<a href="website_files/Aseem_CV.pdf">CV</a> &nbsp;/&nbsp;
<a href="https://github.com/aseembits93">Github</a> &nbsp;/&nbsp;
<a href="https://scholar.google.co.in/citations?user=LJQoO7AAAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp;
<a href="https://www.linkedin.com/in/aseembits93/">LinkedIn</a>

        </p>
        </td>
        <td width="33%">
          <img src="website_files/me.jpeg">
        </td>
      </tr>
      </tbody></table>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tbody><tr>
        <td width="100%" valign="middle">
          <heading>Research Interests</heading>
          <p>
          I am focusing broadly on combining perception and planning for decision making under uncertainty, specifically on perception for bipedal locomotion on <a href="https://www.agilityrobotics.com/robots#cassie">Cassie</a>. In the ancient past, I worked on Protein Structure Prediction and Cancer Genomics. 
          </p>
        </td>
      </tr>
      </tbody></table>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tbody><tr>
        <td width="100%" valign="middle">
          <heading>Other Interests</heading>
          <p>
          I play music - <a href="https://www.youtube.com/channel/UCS-mAkSQheveU4dI78Sv1GQ">Youtube</a>, <a href="https://soundcloud.com/aseem-saxena">Soundcloud</a>. I run, swim and cycle to stay fit.</p>
        </td>
      </tr>
      </tbody></table>	    
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tbody><tr>
        <td>
        <heading>Publications</heading>
        </td>
      </tr>
      </tbody></table>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        
        
        <tbody><tr >
          <td width="25%">

            <div class="one">
                
                <img src="website_files/rsspipeline_160.png">
            </div>                
            
              </td>
              <td valign="top" width="75%">
              <p><a href="https://arxiv.org/pdf/1905.12197.pdf">
        <papertitle>LeTS-Drive: Driving in a Crowd by Learning from Tree Search</papertitle></a><br>
        <strong>Panpan Cai</strong>, Yuanfu Luo, Aseem Saxena, David Hsu, Wee Sun Lee<br>
                <em>RSS (Robotics Science and Systems)</em>, 2019 (Accepted)<br>
                <a href="https://www.youtube.com/watch?v=oghGK3QJFVo">video</a>
		           
                
                
              </p><p></p>
              <p> Autonomous driving in a crowded environment, e.g., a busy traffic intersection, is an unsolved challenge for robotics. We propose
                LeTS-Drive, which integrates online POMDP planning and deep
                learning.</p>
              
              <p></p>
              <p></p>
              </td>
            </tr>
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        
        
              <tbody><tr >
                <td width="25%">
      
                  <div class="one">
                      
                      <img src="website_files/pipeline_160.png">
                  </div>                
                  
                    </td>
                    <td valign="top" width="75%">
                    <p><a href="https://arxiv.org/pdf/1706.03220.pdf">
              <papertitle>Exploring Convolutional Networks for End-to-End Visual Servoing</papertitle></a><br>
              <strong>Aseem Saxena</strong>, <strong>Harit Pandya</strong>, Gourav Kumar, K. Madhava Krishna<br>
                      <em>IEEE ICRA (International Conference on Robotics and Automation)</em>, 2017 (Accepted)<br>
                      <a href="https://www.youtube.com/watch?v=Ve6-3sNBfEM">video</a>
                <a href="https://github.com/aseembits93/VS_CNN">code</a>      
                      
                      
                    </p><p></p>
                    <p> We present an end-to-end learning based approach for visual servoing in diverse scenes where the knowledge of camera parameters and scene geometry is not available apriori. This is achieved by training a convolutional neural network over color images with synchronised camera poses.</p>
                    
                    <p></p>
                    <p></p>
                    </td>
                  </tr>      

        
        
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tbody><tr>
        <td>
        <heading>Projects</heading>
        </td>
      </tr>
      </tbody></table>
      <table width="100%" align="center" border="0" cellpadding="20">
      <tbody>
	<td width="25%">
            <div class="oneone">
                
                  <img src="website_files/safelife.png" style="border-style: none"></div>
                
        </td>  
        <td width="75%" valign="top">
        <p>
          <a href="website_files/avoid.pdf">
          <papertitle>Avoiding Side Effects in Complex Navigation Environments</papertitle>
          </a>
          <br>
          Aseem Saxena, Devin Crowley
        </p>
		<p>We explore methods to train agents to complete tasks and simultaneously avoid side effects in the SafeLife. We demonstrate the effectiveness of MT-DQN, a multi task variant of Deep Q Networks for side effect avoidance.</p>
        </td>
      </tr>      
	<td width="25%">
            <div class="one">
                
                  <img src="website_files/qmdpnet.png" style="border-style: none"></div>
                
        </td>  
        <td width="75%" valign="top">
        <p>
          <a href="https://arxiv.org/pdf/1703.06692.pdf">
          <papertitle>Visualizing QMDPNet</papertitle>
          </a>
          <br>
          <strong>Aseem Saxena</strong>
        </p>
		<p>I created a full fledged GUI Visualizer using Python Tkinter Library to understand the QMDPnet algorithm. I visualize various components of a POMDP such as reward map, belief and value function to get an intuition on how the algorithm works.</p>
        </td>
      </tr>
	      <tr>
        <td width="25%"><img src="website_files/gffrwn_160.png" alt="prl" width="160" height="160"></td>
        <td width="75%" valign="top">
        <p>
          <a href="http://robotics.iiit.ac.in/uploads/Main/Publications/Siva_etal_ICVGIP_14.pdf">
          <papertitle>Guess from Far, Recognize when Near: Searching the
Floor for Small Objects</papertitle>
          </a>
          <br>
          <strong>M Siva Karthik</strong>, Sudhanshu Mittal, K. Madhava Krishna, ICVGIP 2014
        <br>
<a href="https://www.youtube.com/watch?v=CbrLFqt_a9I">video</a>
                
                
              </p><p></p>

<p>          Object recognition is achieved using 3-D Point Cloud data from Kinect sensors and constructing a Bag of Words Model on it. It is trained using a Support Vector Machine Classifier. Object Detection is achieved using segmentation of 2-D images by Markov Random Fields. The implementation is done on a Turtlebot with a Kinect Sensor mounted on top of it.
        </p>
        <p></p>
        </td>
      </tr>
      <tr onmouseout="cvpr2012_stop()" onmouseover="cvpr2012_start()">
        <td width="25%">
            <div class="one">
                <div class="two" id="cvpr2012_image" style="opacity: 0;">
                  <img src="website_files/table_out160.png" style="border-style: none"></div>
                <img src="website_files/table_in160.png" style="border-style: none">
            </div>                
            <script type="text/javascript">
            function cvpr2012_start() {
              document.getElementById('cvpr2012_image').style.opacity = "1";
            }
            function cvpr2012_stop() {
              document.getElementById('cvpr2012_image').style.opacity = "0";
            }
            cvpr2012_stop()
            </script>
        </td>
        <td width="75%" valign="top">
        <p>
          <a href="website_files/tabledeepreport.pdf">
          <papertitle>Deep Learning for Table Interest Point Detection</papertitle>
          </a>
          <br>
          <strong>Aseem Saxena</strong>
        </p>
		<p>I attempt to find interest points or corner points of tables in a scene using cues from
semantic segmentation and vanishing lines. Availabilty of semantic information such as
interest points can help mobile robots navigate in a better way.</p>
        </td>
      </tr>
<tr onmouseout="cvpr12012_stop()" onmouseover="cvpr12012_start()">
        <td width="25%">
            <div class="one">
                <div class="two" id="cvpr12012_image" style="opacity: 0;">
                  <img src="website_files/2_out_160.png" style="border-style: none"></div>
                <img src="website_files/2_in_160.png" style="border-style: none">
            </div>                
            <script type="text/javascript">
            function cvpr12012_start() {
              document.getElementById('cvpr12012_image').style.opacity = "1";
            }
            function cvpr12012_stop() {
              document.getElementById('cvpr12012_image').style.opacity = "0";
            }
            cvpr12012_stop()
            </script>
        </td>
        <td width="75%" valign="top">
        <p>
          <a href="website_files/grabcutreport.pdf">
          <papertitle>Automating GrabCut for Multilabel Image Segmentation</papertitle>
          </a>
          <br>
          <strong>Aseem Saxena</strong>
        </p>
		<p>Performing Image Segmentation for 3 labels without user guidance by learning a GMM
for each label and performing alpha expansion algorithm using MRF2.2 Library.</p>
        </td>
      </tr>

<tr >
        
        
        
      </tbody></table>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tbody><tr>
        <td>
        <br>
        <p align="right">
          Inspired by 
          <a href="https://people.eecs.berkeley.edu/~barron/">this</a>
          
        </p>
        </td>
      </tr>
      </tbody></table>
      
  

</body></html>
