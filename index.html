<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<!-- saved from url=(0040)http://people.eecs.berkeley.edu/~barron/ -->
<html>

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    a {
      color: #1772d0;
      text-decoration: none;
    }

    a:focus,
    a:hover {
      color: #f09228;
      text-decoration: none;
    }

    body,
    td,
    th,
    tr,
    p,
    a {
      font-family: Verdana, Helvetica, sans-serif;
      font-size: 18px
    }

    strong {
      font-family: Verdana, Helvetica, sans-serif;
      font-size: 18px;
    }

    heading {
      font-family: Verdana, Helvetica, sans-serif;
      font-size: 22px;
    }

    papertitle {
      font-family: Verdana, Helvetica, sans-serif;
      font-size: 18px;
      font-weight: 700
    }

    name {
      font-family: Verdana, Helvetica, sans-serif;
      font-size: 32px;
    }

    .one {
      width: 160px;
      height: 160px;
      position: relative;
    }

    .two {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }

    .fade {
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }

    span.highlight {
      background-color: #ffffd0;
    }
  </style>

  <title>Aseem</title>

  <link href="./website_files/css" rel="stylesheet" type="text/css">
</head>

<body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tbody>
      <tr>
        <td>
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td width="67%" valign="middle">
                  <p align="center">
                    <name>Aseem Saxena</name>
                  </p>
                  <p> I am currently pursuing my MS in Artificial Intelligence at Oregon State University. I am
                    fortunate to be advised by <a href="https://web.engr.oregonstate.edu/~afern/">Prof Alan Fern</a>.
                  </p>
                  <p>Until very recently, I worked as a Machine Learning Engineer at Panasonic, Singapore.</p>
                  <p> Before that, I was working as a Researcher at <a href="http://motion.comp.nus.edu.sg/">M2AP Lab,
                      School of Computing, NUS</a>, Singapore under the guidance of <a
                      href="http://www.comp.nus.edu.sg/~dyhsu/">Prof. David Hsu</a>.</p>

                  <p>
                    I did a Dual Major in Biology and Electrical And Electronics Engineering at <a
                      href="http://www.bits-pilani.ac.in/">BITS Pilani</a>, my education was funded by the <a
                      href="http://kvpy.iisc.ernet.in/main/index.htm">KVPY Fellowship</a>. I spent time on my thesis at
                    <a href="http://robotics.iiit.ac.in/">Robotics Research Lab, IIIT Hyderabad</a>, where I was advised
                    by <a href="https://faculty.iiit.ac.in/~mkrishna/">Prof Madhava Krishna</a>.
                  </p>
                  <p align="center">
                    <a href="https://robotvisionxyz.github.io">Blog</a> &nbsp;/&nbsp;
                    <a href="mailto:aseem.bits@gmail.com">Email</a> &nbsp;/&nbsp;
                    <a href="website_files/Aseem_CV.pdf">CV</a> &nbsp;/&nbsp;
                    <a href="https://github.com/aseembits93">Github</a> &nbsp;/&nbsp;
                    <a href="https://scholar.google.co.in/citations?user=LJQoO7AAAAAJ&hl=en">Google Scholar</a>
                    &nbsp;/&nbsp;
                    <a href="https://www.linkedin.com/in/aseembits93/">LinkedIn</a>

                  </p>
                </td>
                <td width="33%">
                  <img src="website_files/me.png">
                </td>
              </tr>
            </tbody>
          </table>
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td width="100%" valign="middle">
                  <heading>Research Interests</heading>
                  <p>
                    I want to build ML systems which work well in data and resource constrained situations.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td width="100%" valign="middle">
                  <heading>Other Interests</heading>
                  <p>
                    I play music - <a href="https://www.youtube.com/channel/UCS-mAkSQheveU4dI78Sv1GQ">Youtube</a>, <a
                      href="https://soundcloud.com/aseem-saxena">Soundcloud</a>. I am an <a href="website_files/certificate731.pdf">amateur triathlete</a>.</p>
                </td>
              </tr>
            </tbody>
          </table>
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td>
                  <heading>Publications</heading>
                </td>
              </tr>
            </tbody>
          </table>
                    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">


            <tbody>
              <tr>
                <td width="25%">



                </td>
                <td valign="top" width="75%">
                  <p><a href="https://arxiv.org/abs/2301.01815">
                      <papertitle>Multi-Task Learning for Temporal Processes: A Case Study on Modeling Plant Cold Hardiness</papertitle>
                    </a><br>
                    Aseem Saxena, Paola Pesantez-Cabrera, Jonathan Magby, Markus Keller, Alan Fern<br>
                    <em>Machine Learning Journal, Springer</em>, 2024 (Under Review)<br>


                  </p>
                  <p></p>
                  <p> We present a real-world case study of multi-task learning (MTL) for temporal process modeling from limited data. Specifically, we investigate multi-task learning for the important agricultural problem of predicting grape and cherry cold hardiness. We investigate multi-task learning (MTL) approaches for combining data, where different tasks correspond to different cultivars. Our results show significant differences between architectures and that certain architectures are able to consistently outperform single-task learning and state-of-the-art scientific models. </p>

                  <p></p>
                  <p></p>
                </td>
              </tr>
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">


            <tbody>
              <tr>
                <td width="25%">



                </td>
                <td valign="top" width="75%">
                  <p><a href="https://arxiv.org/abs/2301.01815">
                      <papertitle>Multi-Task Learning for Budbreak Prediction</papertitle>
                    </a><br>
                    Aseem Saxena, Paola Pesantez-Cabrera, Rohan Ballapragada, Markus Keller, Alan Fern<br>
                    <em>Workshop on AI for Agriculture and Food Systems at AAAI</em>, 2023 (Accepted)<br>


                  </p>
                  <p></p>
                  <p> Grapevine budbreak is a key phenological stage of seasonal development, which serves as a signal
                    for the onset of active growth. This is also when grape plants are most vulnerable to damage from
                    freezing temperatures. Hence, it is important for winegrowers to anticipate the day of budbreak
                    occurrence to protect their vineyards from late spring frost events. This work investigates deep
                    learning for budbreak prediction using data collected for multiple grape cultivars. While some
                    cultivars have over 30 seasons of data others have as little as 4 seasons, which can adversely
                    impact prediction accuracy. To address this issue, we investigate multi-task learning, which
                    combines data across all cultivars to make predictions for individual cultivars. Our main result
                    shows that several variants of multi-task learning are all able to significantly improve prediction
                    accuracy compared to learning for each cultivar independently. </p>

                  <p></p>
                  <p></p>
                </td>
              </tr>

              <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">


                <tbody>
                  <tr>
                    <td width="25%">



                    </td>
                    <td valign="top" width="75%">
                      <p><a href="https://arxiv.org/pdf/2209.10585.pdf">
                          <papertitle>Grape Cold Hardiness Prediction via Multi-Task Learning</papertitle>
                        </a><br>
                        Aseem Saxena, Paola Pesantez-Cabrera, Rohan Ballapragada, Kin-Ho Lam, Alan Fern, Markus
                        Keller<br>
                        <em>Innovative Applications of Artificial Intelligence (IAAI)</em>, 2023 (Accepted)<br>


                      </p>
                      <p></p>
                      <p> Cold temperatures during fall and spring have the potential to cause frost damage to
                        grapevines and other fruit plants, which can significantly decrease harvest yields. we study
                        whether deep-learning models can improve cold hardiness prediction for grapes based on data that
                        has been collected over a 30-year time period. A key challenge is that the amount of data per
                        cultivar is highly variable, with some cultivars having only a small amount. For this purpose,
                        we investigate the use of multi-task learning to leverage data across cultivars in order to
                        improve prediction performance for individual cultivars. </p>

                      <p></p>
                      <p></p>
                    </td>
                  </tr>

                  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">


                    <tbody>
                      <tr>
                        <td width="25%">



                        </td>
                        <td valign="top" width="75%">
                          <p><a href="https://arxiv.org/pdf/2206.11812.pdf">
                              <papertitle>Formalizing the Problem of Side Effect Regularization</papertitle>
                            </a><br>
                            <strong>Alexander Matt Turner</strong>, <strong>Aseem Saxena</strong>, Prasad Tadepalli<br>
                            <em>Equal Contribution</em><br>
                            <em>NeurIPS ML Safety Workshop</em>, 2022 (Accepted)<br>


                          </p>
                          <p></p>
                          <p> AI objectives are often hard to specify properly. Some approaches tackle this problem by
                            regularizing the AI's side effects: Agents must weigh off" how much of a mess they make"
                            with an imperfectly specified proxy objective. We propose a formal criterion for side effect
                            regularization via the assistance game framework. </p>

                          <p></p>
                          <p></p>
                        </td>
                      </tr>
                      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">


                        <tbody>
                          <tr>
                            <td width="25%">



                            </td>
                            <td valign="top" width="75%">
                              <p><a href="https://arxiv.org/pdf/2203.07589.pdf">
                                  <papertitle>Sim-to-Real Learning of Footstep-Constrained Bipedal Dynamic Walking
                                  </papertitle>
                                </a><br>
                                Helei Duan, Ashish Malik, Jeremy Dao, Aseem Saxena, Kevin Green, Jonah Siekmann, Alan
                                Fern, Jonathan Hurst<br>
                                <em>IEEE ICRA (International Conference on Robotics and Automation)</em>, 2022
                                (Accepted)<br>


                              </p>
                              <p></p>
                              <p> we aim to maintain the robust and dynamic nature of learned gaits while also
                                respecting footstep constraints imposed externally. We develop an RL formulation for
                                training dynamic gait controllers that can respond to specified touchdown locations. We
                                then successfully demonstrate simulation and sim-to-real performance on the bipedal
                                robot Cassie. </p>

                              <p></p>
                              <p></p>
                            </td>
                          </tr>

                          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">


                            <tbody>
                              <tr>
                                <td width="25%">

                                  <div class="one">

                                    <!-- <img src="website_files/rsspipeline_160.png"> -->
                                  </div>

                                </td>
                                <td valign="top" width="75%">
                                  <p><a href="https://arxiv.org/pdf/1905.12197.pdf">
                                      <papertitle>LeTS-Drive: Driving in a Crowd by Learning from Tree Search
                                      </papertitle>
                                    </a><br>
                                    Panpan Cai, Yuanfu Luo, Aseem Saxena, David Hsu, Wee Sun Lee<br>
                                    <em>RSS (Robotics Science and Systems)</em>, 2019 (Accepted)<br>
                                    <a href="https://www.youtube.com/watch?v=oghGK3QJFVo">video</a>



                                  </p>
                                  <p></p>
                                  <p> Autonomous driving in a crowded environment, e.g., a busy traffic intersection, is
                                    an unsolved challenge for robotics. We propose
                                    LeTS-Drive, which integrates online POMDP planning and deep
                                    learning.</p>

                                  <p></p>
                                  <p></p>
                                </td>
                              </tr>
                              <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">


                                <tbody>
                                  <tr>
                                    <td width="25%">

                                      <div class="one">

                                        <!-- <img src="website_files/pipeline_160.png"> -->
                                      </div>

                                    </td>
                                    <td valign="top" width="75%">
                                      <p><a href="https://arxiv.org/pdf/1706.03220.pdf">
                                          <papertitle>Exploring Convolutional Networks for End-to-End Visual Servoing
                                          </papertitle>
                                        </a><br>
                                        <strong>Aseem Saxena</strong>, <strong>Harit Pandya</strong>, Gourav Kumar, K.
                                        Madhava Krishna<br>
                                        <em>Equal Contribution</em><br>
                                        <em>IEEE ICRA (International Conference on Robotics and Automation)</em>, 2017
                                        (Accepted)<br>
                                        <a href="https://www.youtube.com/watch?v=Ve6-3sNBfEM">video</a>
                                        <a href="https://github.com/aseembits93/VS_CNN">code</a>


                                      </p>
                                      <p></p>
                                      <p> We present an end-to-end learning based approach for visual servoing in
                                        diverse scenes where the knowledge of camera parameters and scene geometry is
                                        not available apriori. This is achieved by training a convolutional neural
                                        network over color images with synchronised camera poses.</p>

                                      <p></p>
                                      <p></p>
                                    </td>
                                  </tr>



                                  <table width="100%" align="center" border="0"
                                                                        cellspacing="0" cellpadding="20">
                                                                        <tbody>
                                                                            <tr>
                                                                                <td>
                                                                                    <heading>Projects</heading>
                                                                                </td>
                                                                            </tr>
                                                                        </tbody>
                                                                    </table>
                                                                    <table width="100%" align="center" border="0"
                                                                        cellpadding="20">
                                                                        <tbody>
                                                                            <td width="25%">
                                                                                <div class="oneone">

                                                                                    <!-- <img src="website_files/safelife.png" style="border-style: none"> -->
                                                                                </div>

                                                                            </td>
                                                                            <td width="75%" valign="top">
                                                                                <p>
                                                                                    <a href="website_files/avoid.pdf">
                                                                                        <papertitle>Avoiding Side
                                                                                            Effects in Complex
                                                                                            Navigation Environments
                                                                                        </papertitle>
                                                                                    </a>
                                                                                    
                                                                                </p>
                                                                                <p>We explore methods to train agents to
                                                                                    complete tasks and simultaneously
                                                                                    avoid
                                                                                    side effects in the SafeLife. We
                                                                                    demonstrate the effectiveness of
                                                                                    MT-DQN, a
                                                                                    multi task variant of Deep Q
                                                                                    Networks for side effect avoidance.
                                                                                </p>
                                                                            </td>
            </tr>
            <td width="25%">
                <div class="one">

                    <!-- <img src="website_files/qmdpnet.png" style="border-style: none"> -->
                </div>

            </td>
            <td width="75%" valign="top">
                <p>
                    <a href="https://github.com/aseembits93/CS533_DQN">
                        <papertitle>Distributed Q-Learning</papertitle>
                    </a>
                </p>
                <p>We implement a distributed version of DQN via the Ray Distributed Framework.</p>
            </td>
            </tr>
            <td width="25%">
                <div class="one">

                    <!-- <img src="website_files/qmdpnet.png" style="border-style: none"> -->
                </div>

            </td>
            <td width="75%" valign="top">
                <p>
                    <a href="website_files/Offline_RL_in_Bipedal_Robotics.pdf">
                        <papertitle>Offline-RL for Bipedal Robots</papertitle>
                    </a>
                </p>
                <p>Reinforcement Learning requires the entire model of the world or interactive access to the world.
                    However, the world model may not be always known or it may be expensive or unsafe to perform
                    multiple interactions with the world. In such scenarios, we would like to make use of existing
                    transaction data to learn a control policy. This is addressed by a class of algorithms referred to
                    as "Offline Reinforcement Learning". In this work, we study and implement "Behaviour Cloning"(BC),
                    "TD3" and a combination of both "TD3+BC" for offline reinforcement learning. We evaluate them on
                    various syntheic datasets and investigate the performance of each of them on different qualities of
                    datasets. We also attempt to use offline RL for the real-world bipedal robot "Cassie" and introduce
                    various datasets for a bipedal locomotion task</p>
            </td>
            </tr>
            <td width="25%">
                <div class="one">

                    <!-- <img src="website_files/qmdpnet.png" style="border-style: none"> -->
                </div>

            </td>
            <td width="75%" valign="top">
                <p>
                    <a href="website_files/ConvexOptimizationFinalProject.pdf">
                        <papertitle>Studying Robustness of Semi-supervised Visual Features to Adversarial Attacks
                        </papertitle>
                    </a>
                </p>
                <p>Neural Network Verification is an important tool towards gauging robustness to adversaries. In this
                    report, I summarise the work of Salman et al who formulate most past work on LP based neural network
                    verification as a convex relaxation problem. The framework can handle different activation functions
                    and pooling layers and also can handle both primal and dual versions of verification. In my work, I
                    try to evaluate the adversarial robustness of classifiers which are trained to simultaneously
                    classify as well as reconstruct the input. I focus on two domains, image classification on the
                    CIFAR10 dataset and Q-Learning in the OpenAI gym cartpole environment.</p>
            </td>
            </tr>
            <td width="25%">
                <div class="one">

                    <!-- <img src="website_files/qmdpnet.png" style="border-style: none"> -->
                </div>

            </td>
            <td width="75%" valign="top">
                <p>
                    <a
                        href="website_files/Investigating_different_exploration_strategies_for_model_based_reinforcement_learning.pdf">
                        <papertitle>MC Dropout for Efficient Exploration</papertitle>
                    </a>
                </p>
                <p>Agents need to explore the world intelligently so as to discover new skills that are useful to
                    perform downstream tasks. To perform exploration, there have been several methods that
                    have been introduced in literature – however they lack a one-on-one comparison under the same policy
                    setting. There is a discrepancy in terms of whether a model-based or a model-free policy is used to
                    perform exploration and the choice of policy can effect the sample-efficiency of the agent
                    significantly. In this project, we focus on implementing three exploration methods in model-based
                    reinforcement learning setting and thoroughly investigate their qualitative and quantitative
                    performance on the continuous control problem of Point Maze. Our experiments show that while
                    ensemble based Plan2Explore (Sekar et al. 2020) performs the best, a naive and simple method such as
                    Monte Carlo Dropout can perform on par with other exploration based methods.</p>
            </td>
            </tr>
            <td width="25%">
                <div class="one">

                    <!-- <img src="website_files/qmdpnet.png" style="border-style: none"> -->
                </div>

            </td>
            <td width="75%" valign="top">
                <p>
                    <a href="https://arxiv.org/pdf/1703.06692.pdf">
                        <papertitle>Visualizing QMDPNet</papertitle>
                    </a>
                </p>
                <p>I created a full fledged GUI Visualizer using Python Tkinter Library to understand the QMDPnet
                    algorithm. I
                    visualize various components of a POMDP such as reward map, belief and value function to get an
                    intuition on
                    how the algorithm works.</p>
            </td>
            </tr>

            <!-- <tr onmouseout="cvpr2012_stop()" onmouseover="cvpr2012_start()"> -->
            <td width="25%">
                <div class="one">
                    <div class="two" id="cvpr2012_image" style="opacity: 0;">
                        <!-- <img src="website_files/table_out160.png" style="border-style: none"> -->
                    </div>
                    <!-- <img src="website_files/table_in160.png" style="border-style: none"> -->
                </div>
                <script type="text/javascript">
                    function cvpr2012_start() {
                        document.getElementById('cvpr2012_image').style.opacity = "1";
                    }
                    function cvpr2012_stop() {
                        document.getElementById('cvpr2012_image').style.opacity = "0";
                    }
                    cvpr2012_stop()
                </script>
            </td>
            <td width="75%" valign="top">
                <p>
                    <a href="website_files/tabledeepreport.pdf">
                        <papertitle>Deep Learning for Table Interest Point Detection</papertitle>
                    </a>
                </p>
                <p>I attempt to find interest points or corner points of tables in a scene using cues from
                    semantic segmentation and vanishing lines. Availabilty of semantic information such as
                    interest points can help mobile robots navigate in a better way.</p>
            </td>
            </tr>
            <!-- <tr onmouseout="cvpr12012_stop()" onmouseover="cvpr12012_start()"> -->
            <td width="25%">
                <div class="one">
                    <div class="two" id="cvpr12012_image" style="opacity: 0;">
                        <!-- <img src="website_files/2_out_160.png" style="border-style: none"> -->
                    </div>
                    <!-- <img src="website_files/2_in_160.png" style="border-style: none"> -->
                </div>
                <script type="text/javascript">
                    function cvpr12012_start() {
                        document.getElementById('cvpr12012_image').style.opacity = "1";
                    }
                    function cvpr12012_stop() {
                        document.getElementById('cvpr12012_image').style.opacity = "0";
                    }
                    cvpr12012_stop()
                </script>
            </td>
            <td width="75%" valign="top">
                <p>
                    <a href="website_files/grabcutreport.pdf">
                        <papertitle>Automating GrabCut for Multilabel Image Segmentation</papertitle>
                    </a>
                </p>
                <p>Performing Image Segmentation for 3 labels without user guidance by learning a GMM
                    for each label and performing alpha expansion algorithm using MRF2.2 Library.</p>
            </td>
            </tr>

            <tr>



        </tbody>
  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tbody>
      <tr>
        <td>
          <br>
          <p align="right">
            Inspired by
            <a href="https://people.eecs.berkeley.edu/~barron/">this</a>

          </p>
        </td>
      </tr>
    </tbody>
  </table>



</body>

</html>
